# GENIA: Study of gender biases in machine learning models using explainable artificial intelligence

## Summary

Artificial intelligence (AI) systems are already part of our daily lives, making autonomous decisions with a high impact on how we inform ourselves, what entertainment we consume, what medical treatment we receive or what job we are qualified for. These types of decisions are based on the historical analysis of the data that we generate through our devices and actions, and which are used to train machine learning (ML) algorithms. The quantity, quality and representativeness of the data is therefore essential for these algorithms to make accurate predictions. However, these ML algorithms are not exempt from making discriminatory or biased decisions if, for example, under-represented groups or a gender perspective were not taken into account in the data collection and subsequent analysis. To limit such cases, it is necessary to know how AI makes decisions, so that we can detect biases based on race, gender, age, etc. Explainable AI (XAI) can provide methods to identify whether these factors have an impact on the behaviour of ML algorithms, as they seek to explain how AI reaches its conclusions in order to facilitate transparency and trust in its results. This project aims to investigate how XAI methods can be effective in detecting gender bias in ML-based predictive systems. The aim is to study the impact of gender information, or its omission, on AI decision-making in domains of great interest such as medicine or education.

## Activities

- Talk at European Researcher's Night - Artificial Intelligence: Can we trust it? (Córdoba, 1st October 2022).
- Attendance to the [Women in Machine Learning Symposium](https://www.youtube.com/playlist?list=PLQY2H8rRoyvwmodeNmu0jLIPDNlDXGeF-) (Online, 9th December 2022).
- Talk at International Day of Women and Girls in Science (11F) - Asking artificial intelligence to explain (Córdoba, 31st January 2023)
- Tutorial at the [EuADS Summer School Data Science for Explainable and Trustworthy AI](https://www.euads.org/fjkdlasjdiglsmdgkcxjhvckh/euads-summer-school-913/) (Luxembourg, 8th June 2023) - The slides and code examples are available on a dedicated [repository](https://github.com/aurorarq/euads-genderbias).
- Attendance to the [Workshop on Evolutionary Computing and Explainable Artificial Intelligence](https://ecxai.github.io/ecxai/workshop-2023.html) within the Genetic and Evolutionary Computation Conference (GECCO'23) (Online, 16th July 2023).
- Invited participation to the colloquium: "Artificial Intelligence, the great educational challenge of the 21st century" within the [V International Congress on Intercultural and Gender Education: Artificial Intelligence as a threat or opportunity for equality](http://www.uco.es/congresos/cieig/programa/) (Córdoba, 3rd October 2023).

## Results

- [Ciencia Violeta](https://github.com/aurorarq/genia/tree/main/uco-cienciavioleta): 1st Scientific Meeting on Research with a Gender Perspective (Córdoba, 14th February 2023).
- Workshop paper: Exploring gender bias in misclassification with clustering and local explanations. 5th International Workshop on eXplainable Knowledge Discovery in Data Mining organised as part of the European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (Turin, 18th September 2023). Supplementary material available on a dedicated [repository](https://github.com/aurorarq/xkdd23-genderbias).

## Funding

This project is funded by the [University of Córdoba](https://www.uco.es/) within its Annual Research Plan (2022), for a 1-year period (Academic year 2022/2023).
